{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Alexnet with autoencoder.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"707CJdGlfIyW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1414},"outputId":"ede964bf-ff83-455b-8681-9da8efc4308f","executionInfo":{"status":"ok","timestamp":1555390238597,"user_tz":-330,"elapsed":418651,"user":{"displayName":"Gyan prakash singh","photoUrl":"https://lh6.googleusercontent.com/-cwsnOyfCsSY/AAAAAAAAAAI/AAAAAAAAAJU/zEtZ1f4ZPF0/s64/photo.jpg","userId":"08763604823469097634"}}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.svm import SVC \n","from scipy.ndimage import convolve\n","from sklearn import linear_model, datasets, metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import BernoulliRBM\n","from sklearn.pipeline import Pipeline\n","from sklearn.base import clone\n","\n","from keras.layers import Dense, Input, Conv2D, MaxPool2D, UpSampling2D\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","from keras.models import Model\n","from imgaug import augmenters\n","import pandas as pd\n","import keras\n","\n","\n","from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","plt.ion()  \n","\n","!pip install split-folders\n","!rm -rf output\n","\n","!git clone https://github.com/pruvi007/ML_Datasets.git\n","    \n","import split_folders\n","split_folders.ratio('ML_Datasets/UCMerced_LandUse/Images', output=\"output\", seed=1337, ratio=(.8, .2)) \n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(256),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(256),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","data_dir = 'output'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n","                                             shuffle=True, num_workers=4)\n","              for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","save_file_name = 'alexnet-transfer.pt'\n","#len(class_names)\n","\n","model = torchvision.models.alexnet(pretrained=True)\n","\n","from torchsummary import summary\n","model.features\n","#summary(model.features.cuda(), (3,256,256))\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","model = model.features.to(device)\n","\n","batch_size=32\n","train_size, validation_size = 1680, 420\n","def extract_features(phase, sample_count):\n","    features = np.zeros(shape=(sample_count, 256, 7, 7))  # Must be equal to the output of the convolutional base\n","    labels = np.zeros(shape=(sample_count))\n","   \n","    # Pass data through convolutional base\n","    i = 0\n","    for inputs, label in dataloaders[phase]:\n","        inputs = inputs.to(device)\n","        features_batch = model(inputs)\n","        features[i * batch_size: (i + 1) * batch_size] = features_batch.cpu().numpy()\n","        labels[i * batch_size: (i + 1) * batch_size] = label.numpy()\n","        i += 1\n","        if i * batch_size >= sample_count:\n","            break\n","    return features, labels\n","  \n","train_features, train_labels = extract_features('train', train_size) \n","validation_features, validation_labels = extract_features('val', validation_size)\n","\n","def scale(X, eps = 0.001):\n","\t\n","\treturn (X - np.min(X, axis = 0)) / (np.max(X, axis = 0) + eps)\n","\n","X_train, y_train = train_features.reshape(1680,7*7*256), train_labels\n","X_train = scale(X_train)\n","X_test = scale(validation_features.reshape(420,7*7*256))\n","y_test = validation_labels\n","X_test.shape\n","\n","import keras\n","y_train = keras.utils.to_categorical(y_train,21)\n","y_test = keras.utils.to_categorical(y_test,21)\n","\n","## input layer\n","input_layer = Input(shape=(12544,))\n","\n","## encoding architecture\n","encode_layer1 = Dense(200, activation='relu')(input_layer)\n","\n","\n","## latent view\n","latent_view   = Dense(12544, activation='sigmoid')(encode_layer1)\n","\n","\n","\n","## output layer\n","output_layer  = Dense(21, activation='softmax')(latent_view)\n","\n","autoencoder = Model(input_layer,latent_view)\n","\n","autoencoder.compile(optimizer='adam', loss='mse')\n","\n","autoencoder.fit(scale(X_train), scale(X_train), epochs=10, batch_size=32, validation_data=(scale(X_test), scale(X_test) ),verbose=1)\n","\n","model = Model(input_layer, output_layer)\n","model.compile(loss=keras.losses.categorical_crossentropy,optimizer='adam',metrics=['accuracy'])\n","model_log = model.fit(scale(X_train), y_train, epochs=20, batch_size=32, validation_data=(scale(X_test), y_test ),verbose=1)\n","\n","\n","    "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Collecting split-folders\n","  Downloading https://files.pythonhosted.org/packages/ff/95/000c77bad0fbf0825454b7ff8670216449ee01c39e83328c7ce7cd9895c0/split_folders-0.2.1-py3-none-any.whl\n","Installing collected packages: split-folders\n","Successfully installed split-folders-0.2.1\n","Cloning into 'ML_Datasets'...\n","remote: Enumerating objects: 49282, done.\u001b[K\n","remote: Total 49282 (delta 0), reused 0 (delta 0), pack-reused 49282\u001b[K\n","Receiving objects: 100% (49282/49282), 2.16 GiB | 39.39 MiB/s, done.\n","Resolving deltas: 100% (7/7), done.\n","Checking out files: 100% (49247/49247), done.\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.torch/models/alexnet-owt-4df8aa71.pth\n","244418560it [00:03, 71004047.81it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Train on 1680 samples, validate on 420 samples\n","Epoch 1/10\n","1680/1680 [==============================] - 6s 3ms/step - loss: 0.0386 - val_loss: 0.0254\n","Epoch 2/10\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0160 - val_loss: 0.0252\n","Epoch 3/10\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0158 - val_loss: 0.0249\n","Epoch 4/10\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0156 - val_loss: 0.0247\n","Epoch 5/10\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0154 - val_loss: 0.0246\n","Epoch 6/10\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0153 - val_loss: 0.0244\n","Epoch 7/10\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0152 - val_loss: 0.0243\n","Epoch 8/10\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0150 - val_loss: 0.0241\n","Epoch 9/10\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0149 - val_loss: 0.0240\n","Epoch 10/10\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0147 - val_loss: 0.0238\n","Train on 1680 samples, validate on 420 samples\n","Epoch 1/20\n","1680/1680 [==============================] - 6s 3ms/step - loss: 2.1330 - acc: 0.3423 - val_loss: 1.2949 - val_acc: 0.5548\n","Epoch 2/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 1.0499 - acc: 0.6518 - val_loss: 0.7513 - val_acc: 0.7476\n","Epoch 3/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.5130 - acc: 0.8262 - val_loss: 0.6533 - val_acc: 0.7595\n","Epoch 4/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.2301 - acc: 0.9327 - val_loss: 0.5694 - val_acc: 0.8143\n","Epoch 5/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0827 - acc: 0.9851 - val_loss: 0.5073 - val_acc: 0.8405\n","Epoch 6/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0357 - acc: 0.9958 - val_loss: 0.5262 - val_acc: 0.8357\n","Epoch 7/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0265 - acc: 0.9952 - val_loss: 0.5589 - val_acc: 0.8381\n","Epoch 8/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0136 - acc: 0.9988 - val_loss: 0.5828 - val_acc: 0.8262\n","Epoch 9/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5511 - val_acc: 0.8452\n","Epoch 10/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.5826 - val_acc: 0.8429\n","Epoch 11/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.5886 - val_acc: 0.8500\n","Epoch 12/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 8.9786e-04 - acc: 1.0000 - val_loss: 0.5855 - val_acc: 0.8571\n","Epoch 13/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 6.4443e-04 - acc: 1.0000 - val_loss: 0.5977 - val_acc: 0.8595\n","Epoch 14/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 4.9128e-04 - acc: 1.0000 - val_loss: 0.5987 - val_acc: 0.8571\n","Epoch 15/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 3.8298e-04 - acc: 1.0000 - val_loss: 0.6156 - val_acc: 0.8571\n","Epoch 16/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 3.0673e-04 - acc: 1.0000 - val_loss: 0.6182 - val_acc: 0.8571\n","Epoch 17/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 2.5024e-04 - acc: 1.0000 - val_loss: 0.6127 - val_acc: 0.8619\n","Epoch 18/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 2.0353e-04 - acc: 1.0000 - val_loss: 0.6106 - val_acc: 0.8595\n","Epoch 19/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 1.6934e-04 - acc: 1.0000 - val_loss: 0.6198 - val_acc: 0.8571\n","Epoch 20/20\n","1680/1680 [==============================] - 5s 3ms/step - loss: 1.4208e-04 - acc: 1.0000 - val_loss: 0.6199 - val_acc: 0.8643\n"],"name":"stdout"}]},{"metadata":{"id":"mJBirXf3dQYE","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}